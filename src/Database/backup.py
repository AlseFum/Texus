"""
æ•°æ®åº“å¤‡ä»½ç³»ç»Ÿ

åŠŸèƒ½ï¼š
- è‡ªåŠ¨å¤‡ä»½ï¼šæ¯åˆ†é’Ÿä¿å­˜ä¸€æ¬¡æ•°æ®åº“çŠ¶æ€
- å¤‡ä»½ç®¡ç†ï¼šä¿ç•™æœ€æ–° 10 ä¸ªå¤‡ä»½æ–‡ä»¶
- æ•°æ®æ¢å¤ï¼šä»å¤‡ä»½æ–‡ä»¶æ¢å¤æ•°æ®åº“
"""

from datetime import datetime
import json
import asyncio
from pathlib import Path
from protocol.types import File

# å¤‡ä»½é…ç½®
BACKUP_DIR = Path(__file__).parent / "backups"
BACKUP_INTERVAL = 60  # ç§’
MAX_BACKUPS = 10      # ä¿ç•™çš„å¤‡ä»½æ•°é‡


def serialize_table(table) -> dict:
    """åºåˆ—åŒ–è¡¨æ•°æ®
    
    Args:
        table: Table å¯¹è±¡
    
    Returns:
        åºåˆ—åŒ–åçš„å­—å…¸
    """
    serialized = {}
    for key, value in table.inner.items():
        if isinstance(value, File):
            # File å¯¹è±¡è½¬ä¸ºå­—å…¸
            serialized[key] = {
                "_type": "File",
                "mime": value._mime,
                "value": value._value
            }
        elif isinstance(value, dict):
            serialized[key] = {
                "_type": "dict",
                "data": value
            }
        else:
            serialized[key] = {
                "_type": "raw",
                "data": value
            }
    return serialized


def deserialize_table(data: dict, table):
    """ååºåˆ—åŒ–è¡¨æ•°æ®
    
    Args:
        data: åºåˆ—åŒ–çš„æ•°æ®
        table: ç›®æ ‡ Table å¯¹è±¡
    """
    for key, item in data.items():
        if item.get("_type") == "File":
            table.set(key, File(mime=item["mime"], value=item["value"]))
        elif item.get("_type") == "dict":
            table.set(key, item["data"])
        else:
            table.set(key, item.get("data"))


def save_backup():
    """ä¿å­˜å½“å‰æ•°æ®åº“çŠ¶æ€ä¸ºå¤‡ä»½æ–‡ä»¶
    
    Returns:
        å¤‡ä»½æ–‡ä»¶çš„ Path å¯¹è±¡ï¼Œå¤±è´¥åˆ™è¿”å› None
    """
    try:
        from . import tables  # å¯¼å…¥å…¨å±€ tables å­—å…¸
        
        # åˆ›å»ºå¤‡ä»½ç›®å½•
        BACKUP_DIR.mkdir(exist_ok=True)
        
        # ç”Ÿæˆå¤‡ä»½æ–‡ä»¶åï¼ˆæ—¶é—´æˆ³ï¼‰
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        backup_file = BACKUP_DIR / f"db_backup_{timestamp}.bak"
        
        # åºåˆ—åŒ–æ‰€æœ‰è¡¨
        backup_data = {
            "timestamp": timestamp,
            "tables": {}
        }
        
        for table_name, table in tables.items():
            backup_data["tables"][table_name] = serialize_table(table)
        
        # ä¿å­˜åˆ°æ–‡ä»¶
        with open(backup_file, 'w', encoding='utf-8') as f:
            json.dump(backup_data, f, ensure_ascii=False, indent=2, default=str)
        
        print(f"âœ“ æ•°æ®åº“å·²å¤‡ä»½: {backup_file.name}")
        
        # æ¸…ç†æ—§å¤‡ä»½
        cleanup_old_backups()
        
        return backup_file
    
    except Exception as e:
        print(f"âœ— å¤‡ä»½å¤±è´¥: {e}")
        return None


def cleanup_old_backups():
    """æ¸…ç†æ—§å¤‡ä»½ï¼Œåªä¿ç•™æœ€æ–°çš„ MAX_BACKUPS ä¸ª"""
    try:
        # è·å–æ‰€æœ‰å¤‡ä»½æ–‡ä»¶
        backup_files = list(BACKUP_DIR.glob("db_backup_*.bak"))
        
        if len(backup_files) <= MAX_BACKUPS:
            return
        
        # æŒ‰ä¿®æ”¹æ—¶é—´æ’åºï¼ˆæœ€æ–°çš„åœ¨å‰ï¼‰
        backup_files.sort(key=lambda f: f.stat().st_mtime, reverse=True)
        
        # åˆ é™¤å¤šä½™çš„å¤‡ä»½
        for old_backup in backup_files[MAX_BACKUPS:]:
            old_backup.unlink()
            print(f"  - åˆ é™¤æ—§å¤‡ä»½: {old_backup.name}")
    
    except Exception as e:
        print(f"âœ— æ¸…ç†å¤‡ä»½å¤±è´¥: {e}")


def load_latest_backup():
    """åŠ è½½æœ€æ–°çš„å¤‡ä»½
    
    Returns:
        æˆåŠŸè¿”å› Trueï¼Œå¤±è´¥è¿”å› False
    """
    try:
        from . import Table  # å¯¼å…¥ Table ç±»
        
        if not BACKUP_DIR.exists():
            print("â„¹ æ— å¤‡ä»½ç›®å½•")
            return False
        
        # è·å–æ‰€æœ‰å¤‡ä»½æ–‡ä»¶
        backup_files = list(BACKUP_DIR.glob("db_backup_*.bak"))
        
        if not backup_files:
            print("â„¹ æ— å¤‡ä»½æ–‡ä»¶")
            return False
        
        # è·å–æœ€æ–°çš„å¤‡ä»½
        latest_backup = max(backup_files, key=lambda f: f.stat().st_mtime)
        
        # åŠ è½½å¤‡ä»½
        with open(latest_backup, 'r', encoding='utf-8') as f:
            backup_data = json.load(f)
        
        # æ¢å¤æ‰€æœ‰è¡¨
        for table_name, table_data in backup_data["tables"].items():
            table = Table.of(table_name)
            deserialize_table(table_data, table)
        
        print(f"âœ“ å·²ä»å¤‡ä»½æ¢å¤: {latest_backup.name}")
        return True
    
    except Exception as e:
        print(f"âœ— åŠ è½½å¤‡ä»½å¤±è´¥: {e}")
        return False


def list_backups():
    """åˆ—å‡ºæ‰€æœ‰å¤‡ä»½
    
    Returns:
        å¤‡ä»½æ–‡ä»¶ä¿¡æ¯åˆ—è¡¨
    """
    if not BACKUP_DIR.exists():
        return []
    
    backup_files = list(BACKUP_DIR.glob("db_backup_*.bak"))
    backup_files.sort(key=lambda f: f.stat().st_mtime, reverse=True)
    
    return [
        {
            "file": f.name,
            "size": f.stat().st_size,
            "mtime": datetime.fromtimestamp(f.stat().st_mtime).isoformat()
        }
        for f in backup_files
    ]


async def auto_backup_loop():
    """è‡ªåŠ¨å¤‡ä»½å¾ªç¯ä»»åŠ¡"""
    print(f"ğŸ”„ è‡ªåŠ¨å¤‡ä»½å·²å¯åŠ¨ (é—´éš”: {BACKUP_INTERVAL}ç§’, ä¿ç•™: {MAX_BACKUPS}ä¸ª)")
    
    while True:
        try:
            await asyncio.sleep(BACKUP_INTERVAL)
            save_backup()
        except asyncio.CancelledError:
            print("ğŸ›‘ è‡ªåŠ¨å¤‡ä»½å·²åœæ­¢")
            break
        except Exception as e:
            print(f"âœ— è‡ªåŠ¨å¤‡ä»½é”™è¯¯: {e}")


def start_auto_backup():
    """å¯åŠ¨è‡ªåŠ¨å¤‡ä»½ï¼ˆåœ¨ FastAPI å¯åŠ¨æ—¶è°ƒç”¨ï¼‰
    
    Returns:
        asyncio.Task å¯¹è±¡
    """
    return asyncio.create_task(auto_backup_loop())

